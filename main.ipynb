{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuan/miniforge3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "99 2036.2734375\n",
      "199 1404.9825439453125\n",
      "299 971.3155517578125\n",
      "399 673.0686645507812\n",
      "499 467.72637939453125\n",
      "599 326.1922607421875\n",
      "699 228.53268432617188\n",
      "799 161.0756072998047\n",
      "899 114.43142700195312\n",
      "999 82.145751953125\n",
      "1099 59.776222229003906\n",
      "1199 44.26213073730469\n",
      "1299 33.492340087890625\n",
      "1399 26.009098052978516\n",
      "1499 20.804893493652344\n",
      "1599 17.182479858398438\n",
      "1699 14.658984184265137\n",
      "1799 12.89959716796875\n",
      "1899 11.672037124633789\n",
      "1999 10.814900398254395\n",
      "Result: y = -0.04334169626235962 + 0.8393216729164124 x + 0.007477166596800089 x^2 + -0.09085267782211304 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "# Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "class COCOdataset(Dataset):\n",
    "    def __init__(self, root, img_paths, label_paths, img_size=640):\n",
    "        self.root_img = root\n",
    "        self.root_label = root.replace(\"images\", \"labels\")\n",
    "        self.img_paths = img_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.resize = transforms.Resize((img_size, img_size))\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of images and labels in the dataset\n",
    "        assert len(self.img_paths) == len(self.label_paths), \"There is an unmatched number of images and labels!\"\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image and label paths at the given index\n",
    "        image_path = self.img_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "        assert image_path.rstrip(\"jpg\") == label_path.rstrip(\"txt\"), \"Image and label does not match\"\n",
    "\n",
    "        with open(os.path.join(self.root_label, label_path)) as f:\n",
    "            label = np.loadtxt(f)\n",
    "            label = torch.from_numpy(label).float()\n",
    "            if label.dim() == 1:\n",
    "                label = label.unsqueeze(0)\n",
    "            \n",
    "        # image = torchvision.io.read_image(os.path.join(self.root_img, image_path))\n",
    "        image = Image.open(os.path.join(self.root_img, image_path))\n",
    "        image, label = self.pad_and_scale(image, label)\n",
    "        image = self.to_tensor(image)\n",
    "        label = self.pad_lab(label)\n",
    "        return to_pil(image), label, [os.path.join(self.root_img, image_path),os.path.join(self.root_label, label_path)]\n",
    "    \n",
    "    def pad_and_scale(self, img, lab):\n",
    "        w,h = img.size\n",
    "        if w==h:\n",
    "            padded_img = img\n",
    "        else:\n",
    "            dim_to_pad = 1 if w<h else 2\n",
    "            if dim_to_pad == 1:\n",
    "                padding = (h - w) / 2\n",
    "                padded_img = Image.new('RGB', (h,h), color=(127,127,127))\n",
    "                padded_img.paste(img, (int(padding), 0))\n",
    "                lab[:, [1]] = (lab[:, [1]] * w + padding) / h\n",
    "                lab[:, [3]] = (lab[:, [3]] * w / h)\n",
    "            else:\n",
    "                padding = (w - h) / 2\n",
    "                padded_img = Image.new('RGB', (w, w), color=(127,127,127))\n",
    "                padded_img.paste(img, (0, int(padding)))\n",
    "                lab[:, [2]] = (lab[:, [2]] * h + padding) / w\n",
    "                lab[:, [4]] = (lab[:, [4]] * h  / w)\n",
    "        \n",
    "        padded_img = self.resize(padded_img)\n",
    "        return padded_img, lab\n",
    "\n",
    "    def pad_lab(self, lab):\n",
    "        pad_size = 80 - lab.shape[0]\n",
    "        if(pad_size>0):\n",
    "            padded_lab = F.pad(lab, (0, 0, 0, pad_size), value=1)\n",
    "        else:\n",
    "            padded_lab = lab\n",
    "        return padded_lab\n",
    "\n",
    "# Create the dataset\n",
    "root = \"../datasets/car/images/train2017/\"\n",
    "img_paths = os.listdir(root)\n",
    "label_paths = os.listdir(root.replace(\"images\", \"labels\"))\n",
    "dataset = COCOdataset(root, sorted(img_paths), sorted(label_paths), img_size=640)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)\n",
    "\n",
    "img_with_car_no_detected = []\n",
    "for idx, (img, label, image_path) in enumerate(dataset):\n",
    "    res = model(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "279cbe11efe55e230497c765247654ec5b23b2f7308ad2fe56638e781485cf70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
